{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bartalisd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bartalisd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports:\n",
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,CategoricalNB\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn import svm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "import optuna\n",
    "sns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\n",
    "matplotlib.rcParams['figure.figsize'] =[8,8]\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data:\n",
    "with open(\"../data_root.txt\") as f:\n",
    "    data_root_dir = f.read()\n",
    "data_root_dir\n",
    "train = pd.read_csv('%s/tweet_disaster/train.csv' % data_root_dir)\n",
    "test = pd.read_csv('%s/tweet_disaster/test.csv' % data_root_dir)\n",
    "#sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bartalisd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bartalisd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/home/bartalisd/git/apricot_exp/experiments/tweet_disaster/data_analysis.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['text_length'] = train.text.apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "#Analyse & clean data:\n",
    "from data_analysis import analysis, data_cleaning\n",
    "frame, train, vocabulary = analysis(train, test)\n",
    "X, Y = data_cleaning(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test cut:\n",
    "from apricot_exp.evaluation import traintest\n",
    "X_train, X_test, Y_train, Y_test = traintest(X,Y,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5264)\n",
      "(10000, 2257)\n"
     ]
    }
   ],
   "source": [
    "#TFIDF & transpose data:\n",
    "max_num_features = 10000\n",
    "from data_analysis import tfidf\n",
    "features_t, features_test_t = tfidf(X_train, X_test, max_num_features)\n",
    "print(features_t.shape)\n",
    "print(features_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.comet_utils import *\n",
    "api_key = load_api_key('../../comet_key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Config key 'comet.auto_log.weights' is deprecated, please use 'comet.auto_log.histogram_weights' instead\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/apricot/tweet-disaster/c9fdb8d7a7fe47cfaf0de1d4357ed144\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5264, 9000) (2257, 9000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n",
      "COMET INFO: Still uploading\n"
     ]
    }
   ],
   "source": [
    "#Bare model:\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "n = 9000\n",
    "experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "experiment.log_parameters({\n",
    "    'size': None,\n",
    "    'function': None,\n",
    "})\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,max_features=n, min_df=1, norm='l2',  ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(X_train).toarray()\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "print(features.shape, features_test.shape)\n",
    "from apricot_exp.evaluation import train_eval\n",
    "train_eval(model, features,  Y_train, features_test, Y_test, experiment)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.func_tp import featureb, facilityloc, maxcov\n",
    "from comet import suggest_config, extract_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-10-25 08:16:24,356]\u001b[0m A new study created in memory with name: no-name-b3782ffd-5169-4bb8-8a97-84a42d718ba6\u001b[0m\n",
      "\u001b[33m[W 2021-10-25 08:16:24,360]\u001b[0m Trial 0 failed because of the following error: TypeError('featureb() takes 4 positional arguments but 5 were given',)\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bartalisd/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-9-a7cb1b0e44f2>\", line 11, in objective\n",
      "    Xtr_t, Xte_t = featureb(features_t, features_test_t, 100, config[\"function\"], config[\"optimizer\"])\n",
      "TypeError: featureb() takes 4 positional arguments but 5 were given\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "featureb() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a7cb1b0e44f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a7cb1b0e44f2>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuggest_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mXtr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXte_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_test_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mXtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXtr_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mXte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXte_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: featureb() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from optuna.samplers import TPESampler, RandomSampler, GridSampler\n",
    "from parameters import param, algo\n",
    "parameters = param('featurebased')\n",
    "search_alg = \"RND\"\n",
    "algo = algo(search_alg, parameters)\n",
    "    \n",
    "from apricot_exp.evaluation import train_eval\n",
    "\n",
    "def objective(trial):\n",
    "    config = suggest_config(parameters, trial)\n",
    "    Xtr_t, Xte_t = featureb(features_t, features_test_t, 100, config[\"function\"], config[\"optimizer\"])\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    acc, pre, rec, roc = train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    return roc\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=algo)\n",
    "study.optimize(objective, n_trials=3, n_jobs=1)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]:\n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'featurebased'\n",
    "    })\n",
    "    n = i\n",
    "    #start_time = time.time()\n",
    "    Xtr_t, Xte_t = featureb(features_t, features_test_t, n, 'sqrt')\n",
    "    #experiment.log_metric(\"running_time\", time.time()-start_time)\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'facilitylocation'\n",
    "    })\n",
    "    n = i\n",
    "    Xtr_t, Xte_t = facilityloc(features_t, features_test_t, n, 'euclidean')\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    n = i\n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'maxcoverage'\n",
    "    })\n",
    "    from apricot_exp.func_tp import maxcov\n",
    "    Xtr_t, Xte_t = maxcov(features_t, features_test_t, n)\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomtrain(X_tr, X_te, n):\n",
    "    X_train_arr = X_tr.to_numpy()\n",
    "    X_test_arr = X_te.to_numpy() \n",
    "    print(X_train_arr.shape, X_test_arr.shape)\n",
    "    idxs = np.arange(X_tr.shape[0])\n",
    "    np.random.shuffle(idxs)\n",
    "    idx = idxs[:n]\n",
    "    Xtr, Xte = X_train_arr[idx,:], X_test_arr[idx,:]\n",
    "    return Xtr, Xte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    for i in range(1,20):\n",
    "        n = j\n",
    "        experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "        experiment.log_parameters({\n",
    "            'size': i,\n",
    "            'function': 'random'\n",
    "        })\n",
    "        Xtr_t, Xte_t = randomtrain(features_t, features_test_t, n)\n",
    "        Xtr = Xtr_t.transpose()\n",
    "        Xte = Xte_t.transpose()\n",
    "        from apricot_exp.evaluation import train_eval\n",
    "        train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "        i = i+1\n",
    "        experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=20, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.evaluation import train_eval\n",
    "train_eval(model, features,  Y_train, features_test, Y_test, experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "f = list(range(0, 10000))\n",
    "x = features[:, f]\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(x)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "x_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "project = neptune.init(project_qualified_name='bartalisd/nlp-disaster-tweets',\n",
    "             api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNmY5YjdlNTgtNjMxNC00ODQyLThiNGItNmM5NTBmZTM1MjA4In0=', \n",
    "            )\n",
    "\n",
    "# Download experiments dashboard as pandas DataFrame\n",
    "data = project.get_leaderboard()\n",
    "\n",
    "data=data[data['tags'].map(len) > 1]\n",
    "data = data.drop(['name','created', 'finished','owner','notes', 'running_time','size'], axis=1)\n",
    "for i in ['channel_roc', 'channel_acc', 'channel_rec', 'channel_pre']:\n",
    "    data[i]=data[i].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def barplot_res(dataframe, percentage, metric, y_name):\n",
    "    selector = dataframe.apply(lambda x: percentage in x[\"tags\"], axis=1)\n",
    "    data_percentage=dataframe.loc[selector, :]\n",
    "    data_percentage[\"tags\"]=data_percentage[\"tags\"].apply(lambda x: str(x))\n",
    "    res_percentage=data_percentage.groupby(by='tags')[metric].mean().reset_index()\n",
    "    res_percentage['model']=res_percentage['tags'].apply(lambda x: eval(x)[1])\n",
    "    ax=sns.barplot(data=res_percentage, x='model', y=metric)\n",
    "    plt.ylim((0.5, 0.9))\n",
    "    ax.set(xlabel='Model', ylabel=y_name, title =percentage)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "    return ax, res_percentage\n",
    "def lineplot_res(dataframe, y_name, metric, res5k, mod):\n",
    "    data=dataframe.copy()\n",
    "    data['percentage']=data['tags'].apply(lambda x: x[1])\n",
    "    data=data[~data['percentage'].apply(lambda x: 'd' in str(x))]\n",
    "    data['model']=data['tags'].apply(lambda x: x[0])\n",
    "    functions=['facilityloc_d', 'featurebased_d', 'maxcov_d', 'random_d', 'Tfidf']\n",
    "    data=data[data['model'].isin(functions)]\n",
    "    result=data.groupby(by=['model','percentage'])[metric].mean().reset_index()\n",
    "    ax=sns.lineplot(data=result, x='percentage', y=metric, hue='model', palette=['yellow', 'b','r','forestgreen','blueviolet'  ])\n",
    "    ax.set_title(mod,fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "    plt.xlabel('Number of features', fontsize= 15)\n",
    "    plt.ylabel(y_name, fontsize= 15)\n",
    "    plt.axhline(y=res5k, color='brown', linestyle='--')\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize='15')\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Accuracy', 'channel_acc', 0.7984049623393886, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Precision', 'channel_pre', 0.8259162303664922, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Recall', 'channel_rec', 0.6621196222455404, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'ROC AUC', 'channel_roc', 0.8622086211447222, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
