{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) import library & packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,CategoricalNB\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn import svm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "git clone https://github.com/bartalisd/apricot_exp.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%bash\n",
    "cd apricot_exp\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default theme\n",
    "sns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\n",
    "matplotlib.rcParams['figure.figsize'] =[8,8]\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) load data & analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data_root.txt\") as f:\n",
    "    data_root_dir = f.read()\n",
    "data_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('%s/tweet_disaster/train.csv' % data_root_dir)\n",
    "test = pd.read_csv('%s/tweet_disaster/test.csv' % data_root_dir)\n",
    "#sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values=train.isnull().sum()\n",
    "percent_missing = train.isnull().sum()/train.shape[0]*100\n",
    "\n",
    "value = {\n",
    "    'missing_values ':missing_values,\n",
    "    'percent_missing %':percent_missing\n",
    "}\n",
    "frame=pd.DataFrame(value)\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove redundant samples\n",
    "train=train.drop_duplicates(subset=['text', 'target'], keep='first')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 92 redundants sapmles in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "train.groupby('target').id.count().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels are not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers of word for each sapmle in train & test data\n",
    "train['text_length'] = train.text.apply(lambda x: len(x.split()))\n",
    "test['text_length'] = test.text.apply(lambda x: len(x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max number of words in all data is 31 and min is 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all words in single list\n",
    "list_= []\n",
    "for i in train.text:\n",
    "    list_ += i\n",
    "list_= ''.join(list_)\n",
    "allWords=list_.split()\n",
    "vocabulary= set(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 31480 different words in our train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pstem = PorterStemmer()\n",
    "def clean_text(text):\n",
    "    text= text.lower()\n",
    "    text= re.sub('[0-9]', '', text)\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens=[pstem.stem(word) for word in tokens]\n",
    "    # This line below must be enabled to remove stop words (is it because original model needed the stop words too?)\n",
    "    tokens=[word for word in tokens if word not in stopwords.words('english')]\n",
    "    text = ' '.join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"clean\"]=train[\"text\"].apply(clean_text)\n",
    "test[\"clean\"]=test[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the effect of cleaning\n",
    "train[[\"text\",\"clean\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all words in single list\n",
    "list_= []\n",
    "for i in train.clean:\n",
    "    list_ += i\n",
    "list_= ''.join(list_)\n",
    "allWords=list_.split()\n",
    "vocabulary= set(allWords)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we reduced our data from 31480 unique words to 19920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train['clean']\n",
    "Y=train['target']\n",
    "from apricot_exp.evaluation import traintest\n",
    "X_train, X_test, Y_train, Y_test = traintest(X,Y,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True,max_features=max_num_features, min_df=1, norm='l2',  ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(X_train).toarray()\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "print(features.shape, features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_p=pd.DataFrame(features)\n",
    "print(features_p.shape)\n",
    "features_t=features_p.transpose()\n",
    "print(features_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_p=pd.DataFrame(features_test)\n",
    "print(features_test_p.shape)\n",
    "features_test_t=features_test_p.transpose()\n",
    "print(features_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.comet_utils import *\n",
    "api_key = load_api_key('../../comet_key.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 9000\n",
    "experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "experiment.log_parameters({\n",
    "    'size': None,\n",
    "    'function': None\n",
    "})\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,max_features=n, min_df=1, norm='l2',  ngram_range=(1,2))\n",
    "features = tfidf.fit_transform(X_train).toarray()\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "print(features.shape, features_test.shape)\n",
    "from apricot_exp.evaluation import train_eval\n",
    "train_eval(model, features,  Y_train, features_test, Y_test, experiment)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.func_tp import featureb, facilityloc, maxcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]:\n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'featurebased'\n",
    "    })\n",
    "    n = i\n",
    "    #start_time = time.time()\n",
    "    Xtr_t, Xte_t = featureb(features_t, features_test_t, n, 'sqrt')\n",
    "    #experiment.log_metric(\"running_time\", time.time()-start_time)\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'facilitylocation'\n",
    "    })\n",
    "    n = i\n",
    "    Xtr_t, Xte_t = facilityloc(features_t, features_test_t, n, 'euclidean')\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    n = i\n",
    "    experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "    experiment.log_parameters({\n",
    "        'size': i,\n",
    "        'function': 'maxcoverage'\n",
    "    })\n",
    "    from apricot_exp.func_tp import maxcov\n",
    "    Xtr_t, Xte_t = maxcov(features_t, features_test_t, n)\n",
    "    Xtr = Xtr_t.transpose()\n",
    "    Xte = Xte_t.transpose()\n",
    "    from apricot_exp.evaluation import train_eval\n",
    "    train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "    experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomtrain(X_tr, X_te, n):\n",
    "    X_train_arr = X_tr.to_numpy()\n",
    "    X_test_arr = X_te.to_numpy() \n",
    "    print(X_train_arr.shape, X_test_arr.shape)\n",
    "    idxs = np.arange(X_tr.shape[0])\n",
    "    np.random.shuffle(idxs)\n",
    "    idx = idxs[:n]\n",
    "    Xtr, Xte = X_train_arr[idx,:], X_test_arr[idx,:]\n",
    "    return Xtr, Xte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in [100, 500, 1000, 2500, 5000, 7500, 8000, 9000]: \n",
    "    for i in range(1,20):\n",
    "        n = j\n",
    "        experiment=init_experiment(api_key, 'tweet_disaster', 'apricot')\n",
    "        experiment.log_parameters({\n",
    "            'size': i,\n",
    "            'function': 'random'\n",
    "        })\n",
    "        Xtr_t, Xte_t = randomtrain(features_t, features_test_t, n)\n",
    "        Xtr = Xtr_t.transpose()\n",
    "        Xte = Xte_t.transpose()\n",
    "        from apricot_exp.evaluation import train_eval\n",
    "        train_eval(model, Xtr, Y_train, Xte, Y_test, experiment)\n",
    "        i = i+1\n",
    "        experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=20, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apricot_exp.evaluation import train_eval\n",
    "train_eval(model, features,  Y_train, features_test, Y_test, experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "f = list(range(0, 10000))\n",
    "x = features[:, f]\n",
    "x = StandardScaler().fit_transform(x)\n",
    "x = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(x)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "x_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "project = neptune.init(project_qualified_name='bartalisd/nlp-disaster-tweets',\n",
    "             api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNmY5YjdlNTgtNjMxNC00ODQyLThiNGItNmM5NTBmZTM1MjA4In0=', \n",
    "            )\n",
    "\n",
    "# Download experiments dashboard as pandas DataFrame\n",
    "data = project.get_leaderboard()\n",
    "\n",
    "data=data[data['tags'].map(len) > 1]\n",
    "data = data.drop(['name','created', 'finished','owner','notes', 'running_time','size'], axis=1)\n",
    "for i in ['channel_roc', 'channel_acc', 'channel_rec', 'channel_pre']:\n",
    "    data[i]=data[i].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def barplot_res(dataframe, percentage, metric, y_name):\n",
    "    selector = dataframe.apply(lambda x: percentage in x[\"tags\"], axis=1)\n",
    "    data_percentage=dataframe.loc[selector, :]\n",
    "    data_percentage[\"tags\"]=data_percentage[\"tags\"].apply(lambda x: str(x))\n",
    "    res_percentage=data_percentage.groupby(by='tags')[metric].mean().reset_index()\n",
    "    res_percentage['model']=res_percentage['tags'].apply(lambda x: eval(x)[1])\n",
    "    ax=sns.barplot(data=res_percentage, x='model', y=metric)\n",
    "    plt.ylim((0.5, 0.9))\n",
    "    ax.set(xlabel='Model', ylabel=y_name, title =percentage)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "    return ax, res_percentage\n",
    "def lineplot_res(dataframe, y_name, metric, res5k, mod):\n",
    "    data=dataframe.copy()\n",
    "    data['percentage']=data['tags'].apply(lambda x: x[1])\n",
    "    data=data[~data['percentage'].apply(lambda x: 'd' in str(x))]\n",
    "    data['model']=data['tags'].apply(lambda x: x[0])\n",
    "    functions=['facilityloc_d', 'featurebased_d', 'maxcov_d', 'random_d', 'Tfidf']\n",
    "    data=data[data['model'].isin(functions)]\n",
    "    result=data.groupby(by=['model','percentage'])[metric].mean().reset_index()\n",
    "    ax=sns.lineplot(data=result, x='percentage', y=metric, hue='model', palette=['yellow', 'b','r','forestgreen','blueviolet'  ])\n",
    "    ax.set_title(mod,fontdict= { 'fontsize': 20, 'fontweight':'bold'})\n",
    "    plt.xlabel('Number of features', fontsize= 15)\n",
    "    plt.ylabel(y_name, fontsize= 15)\n",
    "    plt.axhline(y=res5k, color='brown', linestyle='--')\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize='15')\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize='20')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Accuracy', 'channel_acc', 0.7984049623393886, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Precision', 'channel_pre', 0.8259162303664922, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'Recall', 'channel_rec', 0.6621196222455404, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineplot_res(data, 'ROC AUC', 'channel_roc', 0.8622086211447222, 'NLP - Disasters tweets: Dim reduction' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
